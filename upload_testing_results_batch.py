#!/usr/bin/env python3
"""
Batch upload script for testing results files.
Processes all files from data/testing_results directory using the same logic
as the frontend CSV uploader.
"""

import os
import sys
import asyncio
from pathlib import Path
import uuid
import csv
import io
import pandas as pd
from typing import Dict, List, Optional
import dotenv

# Add current directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from modules.ai_doctor.ask.ask import Ask
from modules.youtube_summarizer.src.utils.psql_client import PSQLClient
from pydantic import BaseModel
from typing import Optional

# Define TestingResultRow locally to avoid importing api.py (which triggers FastAPI initialization)
class TestingResultRow(BaseModel):
    # id is auto-generated by database, not included in CSV
    test_object: Optional[str] = None
    result_value: Optional[float] = None
    result_unit: Optional[str] = None
    reference_value: Optional[float] = None
    comments: Optional[str] = None
    flag: Optional[str] = None
    testing_date: Optional[str] = None  # DATE format: YYYY-MM-DD
    testing_institution: Optional[str] = None
    testing_location: Optional[str] = None

# Load environment variables
dotenv.load_dotenv()

# Initialize components
psql_client = None
try:
    connection_string = os.getenv("PSQL_CONNECTION_STRING")
    if connection_string:
        psql_client = PSQLClient(connection_string)
        print("‚úÖ Connected to database")
    else:
        print("‚ùå Error: PSQL_CONNECTION_STRING not set")
        sys.exit(1)
except Exception as e:
    print(f"‚ùå Error connecting to database: {e}")
    sys.exit(1)


def run_migrations():
    """Run all necessary migrations for testing_results table."""
    migrations_dir = Path(current_dir) / "modules" / "youtube_summarizer" / "sql" / "migrations"
    
    # List of migrations to run in order
    migration_files = [
        "add_testing_date_column.sql",
        "add_testing_institution_location_columns.sql"
    ]
    
    print(f"\nüìã Running migrations...")
    
    for migration_file_name in migration_files:
        migration_file = migrations_dir / migration_file_name
        
        if not migration_file.exists():
            print(f"‚ö†Ô∏è  Migration file not found: {migration_file_name}, skipping...")
            continue
        
        print(f"\n   Running: {migration_file_name}")
        try:
            with open(migration_file, 'r') as f:
                migration_sql = f.read()
            
            # Execute migration (split by semicolon to handle multiple statements)
            statements = [s.strip() for s in migration_sql.split(';') if s.strip()]
            for statement in statements:
                if statement:
                    try:
                        psql_client.execute_query(statement)
                    except Exception as e:
                        # Check if it's because column already exists
                        if "already exists" in str(e).lower() or "duplicate" in str(e).lower():
                            print(f"      ‚ÑπÔ∏è  Column already exists, skipping...")
                        else:
                            raise
            
            print(f"      ‚úÖ Completed")
        except Exception as e:
            print(f"      ‚ö†Ô∏è  Migration failed: {e}")
            # Continue with other migrations
    
    # Verify all columns exist
    try:
        result = psql_client.read_sql_query(
            "SELECT column_name FROM information_schema.columns "
            "WHERE table_name = 'testing_results' "
            "AND column_name IN ('testing_date', 'testing_institution', 'testing_location')"
        )
        existing_columns = result['column_name'].tolist() if not result.empty else []
        print(f"\n   ‚úÖ Verified columns: {', '.join(existing_columns) if existing_columns else 'none found'}")
        return True
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Could not verify columns: {e}")
        return True  # Continue anyway


async def process_file(file_path: Path) -> Dict:
    """
    Process a single testing results file.
    Returns a dict with success status and details.
    """
    file_name = file_path.name
    print(f"\n{'='*80}")
    print(f"üìÑ Processing: {file_name}")
    print(f"{'='*80}")
    
    result = {
        "file": file_name,
        "success": False,
        "rows_inserted": 0,
        "rows_invalid": 0,
        "error": None
    }
    
    try:
        # Load agent instructions
        instructions_path = Path(current_dir) / "agent" / "sdk" / "testing_results_instructions.txt"
        instructions = ""
        if instructions_path.exists():
            with open(instructions_path, 'r', encoding='utf-8') as f:
                instructions = f.read()
        else:
            result["error"] = f"Instructions file not found: {instructions_path}"
            return result
        
        # Determine file type and construct agent prompt
        file_ext = file_path.suffix.lower()
        if file_ext == '.pdf':
            read_command = f"read_pdf_with_vision('{file_path}')"
        elif file_ext in ['.png', '.jpg', '.jpeg', '.gif', '.webp']:
            read_command = f"read_image('{file_path}')"
        elif file_ext == '.csv':
            read_command = f"read_csv('{file_path}')"
        else:
            read_command = f"read_file('{file_path}')"
        
        # Create agent prompt
        agent_prompt = f"""{instructions}

Now, please read the file at '{file_path}' using the {read_command} tool and extract all testing results. Return ONLY a valid CSV string matching the exact schema specified above, with no additional text or markdown formatting.

IMPORTANT: You MUST extract at least one test result row. If you cannot find any test results, explain why in your response."""
        
        # Run agent - need to use the agent SDK's virtual environment
        agent_sdk_path = os.path.join(current_dir, "agent", "sdk")
        sys.path.insert(0, agent_sdk_path)
        
        # Add the agent SDK's .venv site-packages to path if it exists
        venv_lib_path = os.path.join(agent_sdk_path, ".venv", "lib")
        if os.path.exists(venv_lib_path):
            # Find the Python version directory
            python_dirs = [d for d in os.listdir(venv_lib_path) if d.startswith("python")]
            if python_dirs:
                python_version_dir = python_dirs[0]  # Use the first Python version found
                site_packages_path = os.path.join(venv_lib_path, python_version_dir, "site-packages")
                if os.path.exists(site_packages_path):
                    sys.path.insert(0, site_packages_path)
        
        from file_agent import run_agent_async_with_trace
        
        print(f"ü§ñ Running agent...")
        agent_result = await run_agent_async_with_trace(agent_prompt)
        agent_response = agent_result["final_output"]
        
        print(f"üì§ Agent response received ({len(agent_response)} chars)")
        
        # Extract CSV from response
        csv_content = agent_response.strip()
        
        # Try to extract CSV from markdown code blocks
        if "```csv" in csv_content:
            parts = csv_content.split("```csv")
            if len(parts) > 1:
                csv_content = parts[1].split("```")[0].strip()
        elif "```" in csv_content:
            parts = csv_content.split("```")
            for i, part in enumerate(parts):
                if i % 2 == 1:  # Odd indices are code blocks
                    if "test_object" in part and "," in part:
                        csv_content = part.strip()
                        break
        
        # Find CSV header
        if "test_object" not in csv_content:
            lines = csv_content.split('\n')
            header_line_idx = None
            for idx, line in enumerate(lines):
                if 'test_object' in line.lower() and ',' in line:
                    header_line_idx = idx
                    break
            
            if header_line_idx is not None:
                csv_content = '\n'.join(lines[header_line_idx:])
        
        # Clean up: remove any leading text before the header
        if "test_object," in csv_content:
            lines = csv_content.split('\n')
            for idx, line in enumerate(lines):
                if line.strip().startswith("test_object,"):
                    csv_content = '\n'.join(lines[idx:])
                    break
        
        # Parse CSV and validate with Pydantic
        print(f"üìã Parsing CSV...")
        csv_reader = csv.DictReader(io.StringIO(csv_content))
        rows_list = list(csv_reader)
        
        if len(rows_list) == 0:
            result["error"] = "No data rows found in CSV"
            return result
        
        print(f"üìä Found {len(rows_list)} rows in CSV")
        
        validated_rows = []
        invalid_rows = []
        
        for idx, row in enumerate(rows_list, start=2):  # Start at 2 (header is row 1)
            try:
                # Convert empty strings to None for optional fields
                cleaned_row = {}
                for k, v in row.items():
                    # Skip id if present (auto-generated by database)
                    if k == 'id':
                        continue
                    # Skip reference_unit if present (removed from schema)
                    if k == 'reference_unit':
                        continue
                    cleaned_row[k] = (v if v else None)
                
                # Convert numeric fields - must be valid floats if present
                if cleaned_row.get('result_value'):
                    try:
                        cleaned_row['result_value'] = float(cleaned_row['result_value'])
                    except (ValueError, TypeError):
                        raise ValueError(f"result_value must be a valid float, got: {cleaned_row.get('result_value')}")
                else:
                    cleaned_row['result_value'] = None
                
                if cleaned_row.get('reference_value'):
                    try:
                        cleaned_row['reference_value'] = float(cleaned_row['reference_value'])
                    except (ValueError, TypeError):
                        raise ValueError(f"reference_value must be a valid float, got: {cleaned_row.get('reference_value')}")
                else:
                    cleaned_row['reference_value'] = None
                
                # Validate testing_date format if present (YYYY-MM-DD)
                if cleaned_row.get('testing_date'):
                    import re
                    date_pattern = r'^\d{4}-\d{2}-\d{2}$'
                    if not re.match(date_pattern, cleaned_row['testing_date']):
                        raise ValueError(f"testing_date must be in YYYY-MM-DD format, got: {cleaned_row.get('testing_date')}")
                else:
                    cleaned_row['testing_date'] = None
                
                # Validate with Pydantic
                validated_row = TestingResultRow(**cleaned_row)
                validated_rows.append(validated_row.model_dump())
            except Exception as e:
                invalid_rows.append({
                    "row_number": idx,
                    "error": str(e)
                })
        
        if not validated_rows:
            result["error"] = "No valid rows found in CSV after validation"
            if invalid_rows:
                result["error"] += f" ({len(invalid_rows)} rows were invalid)"
            return result
        
        print(f"‚úÖ Validated {len(validated_rows)} rows")
        if invalid_rows:
            print(f"‚ö†Ô∏è  Skipped {len(invalid_rows)} invalid rows")
        
        # Insert into database
        print(f"üíæ Inserting into database...")
        df = pd.DataFrame(validated_rows)
        
        psql_client.write(
            df,
            "testing_results",
            "public",
            write_disposition="append",
            on_conflict="error"
        )
        
        result["success"] = True
        result["rows_inserted"] = len(validated_rows)
        result["rows_invalid"] = len(invalid_rows)
        print(f"‚úÖ Successfully inserted {len(validated_rows)} rows")
        
    except Exception as e:
        result["error"] = str(e)
        print(f"‚ùå Error processing file: {e}")
        import traceback
        traceback.print_exc()
    
    return result


async def main():
    """Main function to process all files."""
    print("="*80)
    print("üöÄ Batch Testing Results Upload Script")
    print("="*80)
    
    # Run migrations first
    run_migrations()
    
    # Get all files from data/testing_results
    data_dir = Path(current_dir) / "data" / "testing_results"
    
    if not data_dir.exists():
        print(f"‚ùå Directory not found: {data_dir}")
        sys.exit(1)
    
    # Find all PDF, CSV, and image files
    valid_extensions = ['.pdf', '.csv', '.png', '.jpg', '.jpeg', '.gif', '.webp']
    files = [f for f in data_dir.iterdir() if f.is_file() and f.suffix.lower() in valid_extensions]
    
    if not files:
        print(f"‚ùå No valid files found in {data_dir}")
        sys.exit(1)
    
    print(f"\nüìÅ Found {len(files)} files to process")
    
    # Process files sequentially (to avoid overwhelming the API)
    results = []
    for i, file_path in enumerate(files, 1):
        print(f"\n[{i}/{len(files)}] Processing {file_path.name}...")
        result = await process_file(file_path)
        results.append(result)
    
    # Print summary
    print("\n" + "="*80)
    print("üìä UPLOAD SUMMARY")
    print("="*80)
    
    successful = [r for r in results if r["success"]]
    failed = [r for r in results if not r["success"]]
    
    print(f"\n‚úÖ Successful uploads: {len(successful)}")
    total_rows = sum(r["rows_inserted"] for r in successful)
    print(f"   Total rows inserted: {total_rows}")
    
    if successful:
        print("\n   Files:")
        for r in successful:
            print(f"   - {r['file']}: {r['rows_inserted']} rows inserted")
            if r['rows_invalid'] > 0:
                print(f"     (‚ö†Ô∏è  {r['rows_invalid']} invalid rows skipped)")
    
    print(f"\n‚ùå Failed uploads: {len(failed)}")
    if failed:
        print("\n   Files:")
        for r in failed:
            print(f"   - {r['file']}: {r['error']}")
    
    print("\n" + "="*80)


if __name__ == "__main__":
    asyncio.run(main())

