from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, field_validator
from typing import Optional, Dict, List
from sqlalchemy import text
import sys
import os
from pathlib import Path
import csv
import io
import uuid
import pandas as pd
import json

# Add current directory to path for imports
current_dir = os.path.dirname(__file__)
sys.path.insert(0, current_dir)
from modules.ai_doctor.ask.ask import Ask
from modules.ai_doctor.ask.schema_extractor import extract_schema
from modules.youtube_summarizer.src.utils.psql_client import PSQLClient
from modules.matcher.testing_results_unit_converter import TestingResultsUnitConverter
from modules.matcher.testing_object_matcher import TestingObjectMatcher

app = FastAPI()

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify your frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Extract schema on startup
schema_path = Path(__file__).parent / "ask" / "cache" / "db_schema.txt"
try:
    connection_string = os.getenv("PSQL_CONNECTION_STRING")
    if connection_string:
        extract_schema(connection_string, str(schema_path))
        print("Database schema extracted successfully")
    else:
        print("Warning: PSQL_CONNECTION_STRING not set, skipping schema extraction")
except Exception as e:
    print(f"Warning: Failed to extract schema on startup: {e}")

# Initialize Ask instance
ask_instance = Ask()

class PreferencesRequest(BaseModel):
    favoriteChannels: Optional[List[str]] = None
    favoriteSolutions: Optional[List[str]] = None
    pubmedPreferences: Optional[Dict] = None

class AskRequest(BaseModel):
    prompt: str
    preferences: Optional[PreferencesRequest] = None
    max_iterations: Optional[int] = None

class AgentRequest(BaseModel):
    prompt: str
    include_trace: Optional[bool] = False

class TestingResultRow(BaseModel):
    # id is auto-generated by database, not included in CSV
    test_object: Optional[str] = None
    result_value: Optional[float] = None
    result_unit: Optional[str] = None
    reference_value: Optional[float] = None
    comments: Optional[str] = None
    flag: Optional[str] = None
    testing_date: Optional[str] = None  # DATE format: YYYY-MM-DD
    testing_institution: Optional[str] = None
    testing_location: Optional[str] = None

class TestingResultsResponse(BaseModel):
    success: bool
    message: str
    rows_inserted: Optional[int] = None
    rows_invalid: Optional[int] = None
    invalid_row_numbers: Optional[List[int]] = None
    trace: Optional[Dict] = None

class ProcessTestingResultsRequest(BaseModel):
    file_id: str

class UnitConvertRequest(BaseModel):
    unit_category: str
    unit: str
    unit_destination: str
    value: float


@app.post("/ask")
async def ask_endpoint(request: AskRequest):
    try:
        prompt = request.prompt
        preferences = request.preferences.model_dump() if request.preferences else None
        max_iterations = request.max_iterations
        response = ask_instance.ask_directly(
            prompt=prompt, 
            preferences=preferences,
            max_iterations=max_iterations
        )
        return {"response": response}
    except Exception as e:
        return {"error": str(e)}, 500


@app.get("/health")
async def health():
    return {"status": "ok"}

# Initialize PSQL client for testing results
psql_client = None
try:
    connection_string = os.getenv("PSQL_CONNECTION_STRING")
    if connection_string:
        psql_client = PSQLClient(connection_string)
except Exception as e:
    print(f"Warning: Failed to initialize PSQL client: {e}")

@app.post("/agent")
async def agent_endpoint(request: AgentRequest):
    """
    Agent endpoint similar to /ask endpoint.
    Accepts a text prompt and returns agent response.
    Optionally includes trace information if include_trace is True.
    """
    try:
        # Import agent from SDK
        sys.path.insert(0, os.path.join(current_dir, "agent", "sdk"))
        
        if request.include_trace:
            from file_agent import run_agent_async_with_trace
            agent_result = await run_agent_async_with_trace(request.prompt)
            return {
                "response": agent_result["final_output"],
                "trace": {
                    "tool_calls": agent_result.get("tool_calls", []),
                    "tool_usage_summary": agent_result.get("tool_usage_summary", {}),
                    "messages_count": agent_result.get("messages_count", 0),
                    "trace_id": agent_result.get("trace_id")
                }
            }
        else:
            from file_agent import run_agent_async
            response = await run_agent_async(request.prompt)
            return {"response": response}
    except Exception as e:
        return {"error": str(e)}, 500

@app.post("/upload-testing-results")
async def upload_testing_results(file: UploadFile = File(...)):
    """
    Upload a testing results file (PDF, CSV, or image).
    Stores the file in the testing_results directory next to preferences.
    """
    try:
        # Determine file storage location (next to preferences)
        frontend_dist_path = Path(current_dir) / "modules" / "frontend" / "dist"
        testing_results_dir = frontend_dist_path / "testing_results"
        testing_results_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate unique filename
        file_ext = Path(file.filename).suffix
        file_id = str(uuid.uuid4())
        file_path = testing_results_dir / f"{file_id}{file_ext}"
        
        # Save file
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        return {
            "success": True,
            "file_id": file_id,
            "file_path": str(file_path),
            "filename": file.filename
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error uploading file: {str(e)}")

@app.post("/process-testing-results")
async def process_testing_results(request: ProcessTestingResultsRequest):
    """
    Process an uploaded testing results file using the agent.
    Extracts data, validates with Pydantic, and inserts into database.
    """
    try:
        file_id = request.file_id
        
        # Find the uploaded file
        frontend_dist_path = Path(current_dir) / "modules" / "frontend" / "dist"
        testing_results_dir = frontend_dist_path / "testing_results"
        
        # Find file by ID (check common extensions)
        file_path = None
        for ext in ['.pdf', '.csv', '.png', '.jpg', '.jpeg', '.gif', '.webp']:
            candidate = testing_results_dir / f"{file_id}{ext}"
            if candidate.exists():
                file_path = candidate
                break
        
        if not file_path or not file_path.exists():
            raise HTTPException(status_code=404, detail=f"File with ID {file_id} not found")
        
        # Load agent instructions
        instructions_path = Path(current_dir) / "agent" / "sdk" / "testing_results_instructions.txt"
        instructions = ""
        if instructions_path.exists():
            with open(instructions_path, 'r', encoding='utf-8') as f:
                instructions = f.read()
        
        # Determine file type and construct agent prompt
        # NOTE: For PDFs, we now use Vision API (read_pdf_with_vision) instead of read_pdf
        # to handle scanned/image-based PDFs better. The read_pdf tool is kept but deactivated.
        file_ext = file_path.suffix.lower()
        if file_ext == '.pdf':
            read_command = f"read_pdf_with_vision('{file_path}')"
        elif file_ext in ['.png', '.jpg', '.jpeg', '.gif', '.webp']:
            read_command = f"read_image('{file_path}')"
        elif file_ext == '.csv':
            read_command = f"read_csv('{file_path}')"
        else:
            read_command = f"read_file('{file_path}')"
        
        # Create agent prompt
        agent_prompt = f"""{instructions}

Now, please read the file at '{file_path}' using the {read_command} tool and extract all testing results. Return ONLY a valid CSV string matching the exact schema specified above, with no additional text or markdown formatting.

IMPORTANT: You MUST extract at least one test result row. If you cannot find any test results, explain why in your response."""
        
        print(f"\nüìù [AGENT PROMPT] Created agent prompt")
        print(f"   üìè Prompt length: {len(agent_prompt)} characters")
        print(f"   üìÑ File to process: {file_path}")
        print(f"   üîß Read command: {read_command}")
        
        # Run agent with trace information
        sys.path.insert(0, os.path.join(current_dir, "agent", "sdk"))
        from file_agent import run_agent_async_with_trace
        
        try:
            print(f"\nü§ñ [AGENT EXECUTION] Starting agent...")
            agent_result = await run_agent_async_with_trace(agent_prompt)
            agent_response = agent_result["final_output"]
            
            print(f"\nüì§ [AGENT RESPONSE] Received agent response")
            print(f"   üìè Response length: {len(agent_response)} characters")
            print(f"   üìã Response type: {type(agent_response)}")
            print(f"   üìÑ Full response:\n{'='*80}\n{agent_response}\n{'='*80}")
            
            # Check if response is empty or only contains header
            if len(agent_response.strip()) < 100:
                print(f"\n‚ö†Ô∏è  WARNING: Agent response is very short ({len(agent_response)} chars)")
                print(f"   This might indicate the agent didn't extract any data")
            
            trace_info = {
                "tool_calls": agent_result.get("tool_calls", []),
                "tool_usage_summary": agent_result.get("tool_usage_summary", {}),
                "messages_count": agent_result.get("messages_count", 0),
                "trace_id": agent_result.get("trace_id")
            }
        except Exception as e:
            error_msg = f"Agent execution failed: {str(e)}"
            print(f"\n‚ùå Agent Error: {error_msg}")
            import traceback
            print(f"   Traceback:\n{traceback.format_exc()}")
            raise HTTPException(
                status_code=500,
                detail=error_msg
            )
        
        # Extract CSV from response (remove markdown code blocks if present)
        print(f"\nüìÑ [CSV EXTRACTION] Extracting CSV from agent response")
        print(f"   üìè Agent response length: {len(agent_response)} characters")
        print(f"   üìã Full agent response:\n{'='*80}\n{agent_response}\n{'='*80}")
        
        csv_content = agent_response.strip()
        
        # Try to extract CSV from markdown code blocks
        if "```csv" in csv_content:
            print(f"   üîç Found markdown CSV block (```csv)")
            parts = csv_content.split("```csv")
            if len(parts) > 1:
                csv_content = parts[1].split("```")[0].strip()
                print(f"   ‚úÖ Extracted CSV from markdown block")
        elif "```" in csv_content:
            print(f"   üîç Found generic markdown code block (```)")
            # Look for CSV content between code blocks
            parts = csv_content.split("```")
            for i, part in enumerate(parts):
                if i % 2 == 1:  # Odd indices are code blocks
                    if "test_object" in part and "," in part:
                        csv_content = part.strip()
                        print(f"   ‚úÖ Extracted CSV from code block")
                        break
        
        # If still no CSV header found, try to find lines starting with CSV header
        if "test_object" not in csv_content:
            print(f"   ‚ö†Ô∏è  'test_object' not found in content, searching for header pattern...")
            # Look for CSV header pattern in the response
            lines = csv_content.split('\n')
            header_line_idx = None
            for idx, line in enumerate(lines):
                if 'test_object' in line.lower() and ',' in line:
                    header_line_idx = idx
                    print(f"   ‚úÖ Found header at line {idx}: {line[:100]}")
                    break
            
            if header_line_idx is not None:
                csv_content = '\n'.join(lines[header_line_idx:])
            else:
                print(f"   ‚ùå No CSV header found in response")
        
        # Clean up: remove any leading text before the header
        # Look for header starting with test_object (id column is optional/removed)
        if "test_object," in csv_content:
            # Find the header line
            lines = csv_content.split('\n')
            for idx, line in enumerate(lines):
                if line.strip().startswith("test_object,"):
                    csv_content = '\n'.join(lines[idx:])
                    print(f"   ‚úÖ Cleaned CSV starting from header line {idx}")
                    break
        
        print(f"\n   üìã Final CSV content to parse ({len(csv_content)} chars):")
        print(f"   {'='*80}")
        print(csv_content[:1000] if len(csv_content) > 1000 else csv_content)
        print(f"   {'='*80}")
        
        # Parse CSV and validate with Pydantic
        print(f"\nüìã [CSV PARSING] Parsing CSV content")
        print(f"   üìç Stage: Parsing and validating CSV")
        print(f"   üìè CSV content length: {len(csv_content)} characters")
        print(f"   üìã CSV content lines: {len(csv_content.split(chr(10)))}")
        
        try:
            csv_reader = csv.DictReader(io.StringIO(csv_content))
            # Log the detected columns
            if csv_reader.fieldnames:
                print(f"   üìä Detected CSV columns: {csv_reader.fieldnames}")
            else:
                print(f"   ‚ö†Ô∏è  No columns detected in CSV!")
            print(f"   ‚úÖ CSV parsed successfully")
        except Exception as e:
            # If CSV parsing fails, return error with proper status code
            error_msg = f"Failed to parse CSV: {str(e)}. Agent response preview: {agent_response[:500]}"
            print(f"\n‚ùå CSV Parsing Error: {error_msg}")
            raise HTTPException(
                status_code=422, 
                detail=error_msg
            )
        validated_rows = []
        invalid_rows = []
        
        # Convert csv_reader to list to check if it's empty and allow multiple iterations
        rows_list = list(csv_reader)
        print(f"   üìä Total rows read from CSV: {len(rows_list)}")
        
        if len(rows_list) == 0:
            print(f"   ‚ö†Ô∏è  WARNING: CSV reader returned 0 rows!")
            print(f"   üìã CSV content was:\n{csv_content}")
            raise HTTPException(
                status_code=422,
                detail=f"No data rows found in CSV. CSV content: {csv_content[:200]}"
            )
        
        print(f"   üîç Validating rows with Pydantic...")
        for idx, row in enumerate(rows_list, start=2):  # Start at 2 (header is row 1)
            print(f"   üìù Processing row {idx}: {dict(row)}")
            try:
                # Convert empty strings to None for optional fields (except numeric fields)
                cleaned_row = {}
                for k, v in row.items():
                    # Skip id if present (auto-generated by database)
                    if k == 'id':
                        continue
                    # Skip reference_unit if present (removed from schema)
                    if k == 'reference_unit':
                        continue
                    cleaned_row[k] = (v if v else None)
                
                # Convert numeric fields - must be valid floats if present
                if cleaned_row.get('result_value'):
                    try:
                        cleaned_row['result_value'] = float(cleaned_row['result_value'])
                    except (ValueError, TypeError) as e:
                        raise ValueError(f"result_value must be a valid float, got: {cleaned_row.get('result_value')}")
                else:
                    cleaned_row['result_value'] = None
                
                if cleaned_row.get('reference_value'):
                    try:
                        cleaned_row['reference_value'] = float(cleaned_row['reference_value'])
                    except (ValueError, TypeError) as e:
                        raise ValueError(f"reference_value must be a valid float, got: {cleaned_row.get('reference_value')}")
                else:
                    cleaned_row['reference_value'] = None
                
                # Validate testing_date format if present (YYYY-MM-DD)
                if cleaned_row.get('testing_date'):
                    import re
                    date_pattern = r'^\d{4}-\d{2}-\d{2}$'
                    if not re.match(date_pattern, cleaned_row['testing_date']):
                        raise ValueError(f"testing_date must be in YYYY-MM-DD format, got: {cleaned_row.get('testing_date')}")
                else:
                    cleaned_row['testing_date'] = None
                
                # Validate with Pydantic
                validated_row = TestingResultRow(**cleaned_row)
                validated_rows.append(validated_row.model_dump())
                if idx % 10 == 0 or idx == 2:  # Log every 10th row or first row
                    print(f"   ‚úÖ Validated row {idx}")
            except Exception as e:
                # Collect invalid row info but continue processing
                invalid_row_info = {
                    "row_number": idx,
                    "error": str(e),
                    "row_data": dict(row)  # Store original row data for logging
                }
                invalid_rows.append(invalid_row_info)
                print(f"   ‚ö†Ô∏è  Row {idx} INVALID - will be skipped: {str(e)}")
                # Log the problematic row data
                test_obj = row.get('test_object', 'N/A')
                result_val = row.get('result_value', 'N/A')
                print(f"      Row data: test_object='{test_obj}', result_value='{result_val}'")
        
        # Log summary of invalid rows
        if invalid_rows:
            print(f"\n‚ö†Ô∏è  VALIDATION SUMMARY - Invalid Rows Removed:")
            print(f"   Total invalid rows: {len(invalid_rows)}")
            print(f"   Invalid row numbers: {[r['row_number'] for r in invalid_rows]}")
            for invalid_row in invalid_rows:
                print(f"   ‚ùå Row {invalid_row['row_number']}: {invalid_row['error']}")
                test_obj = invalid_row['row_data'].get('test_object', 'N/A')
                result_val = invalid_row['row_data'].get('result_value', 'N/A')
                print(f"      Data: test_object='{test_obj}', result_value='{result_val}'")
        
        if not validated_rows:
            error_msg = "No valid rows found in CSV after validation"
            print(f"\n‚ùå Validation Error: {error_msg}")
            if invalid_rows:
                print(f"   All {len(invalid_rows)} rows were invalid and removed")
            raise HTTPException(
                status_code=422,
                detail=error_msg
            )
        
        # Log success summary
        print(f"\n‚úÖ VALIDATION SUMMARY - Valid Rows:")
        print(f"   Total valid rows: {len(validated_rows)}")
        if invalid_rows:
            print(f"   Total invalid rows removed: {len(invalid_rows)}")
            print(f"   Success rate: {len(validated_rows)}/{len(validated_rows) + len(invalid_rows)} ({100 * len(validated_rows) / (len(validated_rows) + len(invalid_rows)):.1f}%)")
        
        # Insert into database
        print(f"\nüíæ [DATABASE WRITE] psql_client.write()")
        print(f"   üìç Stage: Writing to PostgreSQL database")
        if not psql_client:
            print(f"   ‚ùå Error: Database connection not available")
            raise HTTPException(status_code=500, detail="Database connection not available")
        
        print(f"   üìä Preparing DataFrame with {len(validated_rows)} rows")
        df = pd.DataFrame(validated_rows)
        print(f"   üìã DataFrame columns: {', '.join(df.columns.tolist())}")
        print(f"   üìè DataFrame shape: {df.shape[0]} rows √ó {df.shape[1]} columns")
        
        print(f"\n   üíæ Database Write Parameters:")
        print(f"      Table: public.testing_results")
        print(f"      Write disposition: append")
        print(f"      Conflict handling: error (all rows will be inserted)")
        print(f"      Rows to insert: {len(df)}")
        print(f"      Note: 'id' column will be auto-generated by database")
        
        print(f"   üîå Connecting to database...")
        try:
            # Note: id is auto-generated, so we use 'error' conflict handling which doesn't require index_columns
            # This will insert all rows (duplicates allowed since id is auto-generated)
            psql_client.write(
                df,
                "testing_results",
                "public",
                write_disposition="append",
                on_conflict="error"
            )
            print(f"   ‚úÖ Successfully inserted {len(validated_rows)} rows into database")
            print(f"   üìä Database operation completed")
        except Exception as e:
            print(f"   ‚ùå Database write error: {str(e)}")
            import traceback
            print(f"   üìã Traceback:\n{traceback.format_exc()}")
            raise HTTPException(
                status_code=500,
                detail=f"Database write failed: {str(e)}"
            )
        
        # Log final statistics
        print("\n" + "=" * 80)
        print("üìä API RESPONSE STATISTICS")
        print("=" * 80)
        print(f"‚úÖ Success: True")
        print(f"üìù Message: Successfully inserted {len(validated_rows)} rows")
        print(f"üìä Rows Inserted: {len(validated_rows)}")
        if invalid_rows:
            print(f"‚ö†Ô∏è  Invalid Rows Removed: {len(invalid_rows)}")
            print(f"üìã Invalid Row Numbers: {[r['row_number'] for r in invalid_rows]}")
        print(f"üîß Tool Calls: {len(trace_info.get('tool_calls', []))}")
        print(f"üì® Messages: {trace_info.get('messages_count', 0)}")
        if trace_info.get('tool_usage_summary'):
            print(f"üìã Tool Usage: {json.dumps(trace_info['tool_usage_summary'], indent=2)}")
        print("=" * 80 + "\n")
        
        response_message = f"Successfully inserted {len(validated_rows)} rows"
        if invalid_rows:
            response_message += f" ({len(invalid_rows)} invalid rows were removed)"
        
        return {
            "success": True,
            "message": response_message,
            "rows_inserted": len(validated_rows),
            "rows_invalid": len(invalid_rows) if invalid_rows else 0,
            "invalid_row_numbers": [r['row_number'] for r in invalid_rows] if invalid_rows else [],
            "trace": trace_info
        }
        
    except HTTPException:
        raise
    except Exception as e:
        error_msg = f"Error processing file: {str(e)}"
        print(f"\n‚ùå Unexpected Error: {error_msg}")
        import traceback
        print(f"   Traceback:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=error_msg)

# Initialize unit converter
unit_converter = None
try:
    connection_string = os.getenv("PSQL_CONNECTION_STRING")
    if connection_string:
        unit_converter = TestingResultsUnitConverter(connection_string)
except Exception as e:
    print(f"Warning: Failed to initialize unit converter: {e}")

# Initialize PSQL client for insights endpoints
psql_client_insights = None
try:
    connection_string = os.getenv("PSQL_CONNECTION_STRING")
    if connection_string:
        psql_client_insights = PSQLClient(connection_string)
except Exception as e:
    print(f"Warning: Failed to initialize PSQL client for insights: {e}")

# Initialize TestingObjectMatcher for transforming test_object values
testing_object_matcher = None
try:
    connection_string = os.getenv("PSQL_CONNECTION_STRING")
    if connection_string:
        testing_object_matcher = TestingObjectMatcher()
except Exception as e:
    print(f"Warning: Failed to initialize TestingObjectMatcher: {e}")

@app.post("/testing_results/units/convert")
async def convert_unit(request: UnitConvertRequest):
    """
    Convert a value from one unit to another within the same category.
    
    Args:
        request: UnitConvertRequest with unit_category, unit, unit_destination, and value
        
    Returns:
        Converted value or None if conversion is not possible
    """
    try:
        if not unit_converter:
            raise HTTPException(status_code=500, detail="Unit converter not available")
        
        result = unit_converter.convert(
            unit_category=request.unit_category,
            unit=request.unit,
            unit_destination=request.unit_destination,
            value=request.value
        )
        
        if result is None:
            return {
                "success": False,
                "message": "Conversion not possible (unit may have can_convert=False or conversion not supported)",
                "value": None
            }
        
        return {
            "success": True,
            "value": result,
            "unit_category": request.unit_category,
            "unit": request.unit,
            "unit_destination": request.unit_destination,
            "original_value": request.value
        }
    except Exception as e:
        error_msg = f"Error converting unit: {str(e)}"
        print(f"\n‚ùå Unit Conversion Error: {error_msg}")
        import traceback
        print(f"   Traceback:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=error_msg)

@app.get("/insights/testing-results/available-objects")
async def get_available_testing_objects():
    """
    Get list of unique testing objects from the testing_results table.
    Values are normalized using TestingObjectMatcher before being returned.
    
    Returns:
        List of unique normalized test_object values (excluding NULL values)
    """
    try:
        if not psql_client_insights:
            raise HTTPException(status_code=500, detail="Database connection not available")
        
        if not testing_object_matcher:
            raise HTTPException(status_code=500, detail="TestingObjectMatcher not available")
        
        # Query unique test_object values where testing_date is not NULL
        query = text("""
            SELECT DISTINCT test_object 
            FROM testing_results 
            WHERE test_object IS NOT NULL 
            AND testing_date IS NOT NULL
            ORDER BY test_object
        """)
        
        with psql_client_insights.engine.connect() as connection:
            result = connection.execute(query)
            test_objects = [row[0] for row in result.fetchall()]
        
        # Transform test_object values using TestingObjectMatcher
        if test_objects:
            df = pd.DataFrame({"test_object": test_objects})
            df = testing_object_matcher._normalize_test_object(df, column="test_object")
            # Get unique normalized values (in case multiple original values map to same normalized value)
            normalized_objects = df["normalized_test_object"].dropna().unique().tolist()
            normalized_objects.sort()
        else:
            normalized_objects = []
        
        return {
            "success": True,
            "test_objects": normalized_objects
        }
    except Exception as e:
        error_msg = f"Error fetching available testing objects: {str(e)}"
        print(f"\n‚ùå Error: {error_msg}")
        import traceback
        print(f"   Traceback:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=error_msg)

@app.get("/insights/testing-results/")
async def get_insights_data(testing_object: str):
    """
    Get chart data for a specific testing object over time.
    The testing_object parameter should be the normalized value (as returned by available-objects endpoint).
    Returns normalized_test_object in the response.
    
    Args:
        testing_object: The normalized name of the test object to visualize
        
    Returns:
        Dictionary with:
        - x_values: List of dates (testing_date)
        - y_values: List of result values
        - unit_label: Unified unit label (picks first available, logs warning if multiple)
        - normalized_test_object: The normalized test object name
        - reference_value: Reference value in the main unit (if available)
    """
    try:
        if not psql_client_insights:
            raise HTTPException(status_code=500, detail="Database connection not available")
        
        if not testing_object_matcher:
            raise HTTPException(status_code=500, detail="TestingObjectMatcher not available")
        
        # Query all test_object values that might match (we'll filter by normalized value)
        query = text("""
            SELECT test_object, testing_date, result_value, result_unit
            FROM testing_results
            WHERE test_object IS NOT NULL
            AND testing_date IS NOT NULL
            AND result_value IS NOT NULL
            ORDER BY testing_date ASC
        """)
        
        with psql_client_insights.engine.connect() as connection:
            result = connection.execute(query)
            all_rows = result.fetchall()
        
        if not all_rows:
            # Get reference value even if no data
            reference_value = testing_object_matcher._main_reference.get(testing_object, None) if testing_object_matcher else None
            return {
                "success": True,
                "x_values": [],
                "y_values": [],
                "unit_label": None,
                "normalized_test_object": testing_object,
                "reference_value": reference_value,
                "message": f"No data found for testing object '{testing_object}' with valid testing_date"
            }
        
        # Transform test_object values to find matching rows
        unique_test_objects = list(set([row[0] for row in all_rows]))
        df_mapping = pd.DataFrame({"test_object": unique_test_objects})
        df_mapping = testing_object_matcher._normalize_test_object(df_mapping, column="test_object")
        
        # Create mapping from original to normalized
        mapping_dict = dict(zip(df_mapping["test_object"], df_mapping["normalized_test_object"]))
        
        # Filter rows where normalized value matches the requested testing_object
        # Build DataFrame with matching rows
        matching_data = []
        for row in all_rows:
            original_test_object = row[0]
            normalized_value = mapping_dict.get(original_test_object, original_test_object)
            if normalized_value == testing_object:
                matching_data.append({
                    "normalized_test_object": normalized_value,
                    "testing_date": row[1],
                    "result_value": row[2],
                    "result_unit": row[3]
                })
        
        if not matching_data:
            # Get reference value even if no matching data
            reference_value = testing_object_matcher._main_reference.get(testing_object, None)
            return {
                "success": True,
                "x_values": [],
                "y_values": [],
                "unit_label": None,
                "normalized_test_object": testing_object,
                "reference_value": reference_value,
                "message": f"No data found for normalized testing object '{testing_object}' with valid testing_date"
            }
        
        # Convert to DataFrame for unit filtering
        df_matching = pd.DataFrame(matching_data)
        
        # Apply unit filtering and conversion using _filter_main_unit
        df_filtered = testing_object_matcher._filter_main_unit(df_matching)
        
        if df_filtered.empty:
            # Get reference value even if no data after filtering
            reference_value = testing_object_matcher._main_reference.get(testing_object, None)
            return {
                "success": True,
                "x_values": [],
                "y_values": [],
                "unit_label": None,
                "normalized_test_object": testing_object,
                "reference_value": reference_value,
                "message": f"No data found for normalized testing object '{testing_object}' after unit filtering"
            }
        
        # Extract dates and values from filtered DataFrame
        x_values = [str(date) for date in df_filtered["testing_date"].tolist()]
        y_values = df_filtered["result_value"].tolist()
        
        # Get unique units (should all be the same after filtering/conversion)
        units = df_filtered["result_unit"].dropna().unique().tolist()
        unit_label = units[0] if units else None
        
        if len(units) > 1:
            import logging
            logging.warning(
                f"Multiple unit labels found for testing object '{testing_object}' after filtering: {units}. "
                f"Using '{unit_label}' as unified unit."
            )
            print(f"‚ö†Ô∏è  Warning: Multiple unit labels found for '{testing_object}' after filtering: {units}. Using '{unit_label}'.")
        
        # Get reference value from _main_reference mapping
        reference_value = testing_object_matcher._main_reference.get(testing_object, None)
        
        return {
            "success": True,
            "x_values": x_values,
            "y_values": y_values,
            "unit_label": unit_label,
            "normalized_test_object": testing_object,
            "reference_value": reference_value
        }
    except Exception as e:
        error_msg = f"Error fetching insights data: {str(e)}"
        print(f"\n‚ùå Error: {error_msg}")
        import traceback
        print(f"   Traceback:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=error_msg)

@app.get("/testing-results")
async def get_testing_results():
    """
    Get all testing results from the testing_results table.
    Returns all columns including both original test_object and normalized_test_object.
    
    Returns:
        List of testing result objects with all columns plus normalized_test_object
    """
    try:
        if not psql_client_insights:
            raise HTTPException(status_code=500, detail="Database connection not available")
        
        if not testing_object_matcher:
            raise HTTPException(status_code=500, detail="TestingObjectMatcher not available")
        
        # Query all columns from testing_results table
        query = text("""
            SELECT 
                id,
                test_object,
                result_value,
                result_unit,
                reference_value,
                comments,
                flag,
                testing_date,
                testing_institution,
                testing_location,
                updated_at,
                updated_at_date
            FROM testing_results
            ORDER BY id DESC
        """)
        
        with psql_client_insights.engine.connect() as connection:
            result = connection.execute(query)
            rows = result.fetchall()
        
        if not rows:
            return {
                "success": True,
                "results": []
            }
        
        # Convert rows to list of dictionaries with proper date/timestamp handling
        columns = [
            "id", "test_object", "result_value", "result_unit", "reference_value",
            "comments", "flag", "testing_date", "testing_institution", "testing_location",
            "updated_at", "updated_at_date"
        ]
        
        results = []
        for row in rows:
            row_dict = {}
            for i, col in enumerate(columns):
                value = row[i]
                # Convert dates and timestamps to strings for JSON serialization
                if value is not None and col in ["testing_date", "updated_at_date"]:
                    row_dict[col] = str(value) if hasattr(value, '__str__') else value
                elif value is not None and col == "updated_at":
                    row_dict[col] = str(value) if hasattr(value, '__str__') else value
                else:
                    row_dict[col] = value
            results.append(row_dict)
        
        # Transform test_object values using TestingObjectMatcher
        if results:
            df = pd.DataFrame(results)
            df = testing_object_matcher._normalize_test_object(df, column="test_object")
            
            # Convert DataFrame back to list of dictionaries
            # Include both original test_object and normalized_test_object
            transformed_results = []
            for _, row in df.iterrows():
                result_dict = {}
                for col in df.columns:
                    value = row[col]
                    # Handle NaN values (convert to None for JSON)
                    if pd.isna(value):
                        result_dict[col] = None
                    else:
                        result_dict[col] = value
                transformed_results.append(result_dict)
            
            return {
                "success": True,
                "results": transformed_results
            }
        else:
            return {
                "success": True,
                "results": []
            }
    except Exception as e:
        error_msg = f"Error fetching testing results: {str(e)}"
        print(f"\n‚ùå Error: {error_msg}")
        import traceback
        print(f"   Traceback:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=error_msg)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=3002)

